

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Diverse Counterfactual Explanations (DiCE) for ML &mdash; DiCE 0.7 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="notebooks/nb_index.html" />
    <link rel="prev" title="Diverse Counterfactual Explanations (DiCE) for ML" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> DiCE
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Diverse Counterfactual Explanations (DiCE) for ML</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installing-dice">Installing DICE</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started-with-dice">Getting started with DiCE</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-methods-for-generating-counterfactuals">Supported methods for generating counterfactuals</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-use-cases">Supported use-cases</a></li>
<li class="toctree-l2"><a class="reference internal" href="#feasibility-of-counterfactual-explanations">Feasibility of counterfactual explanations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-promise-of-counterfactual-explanations">The promise of counterfactual explanations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#roadmap">Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="#citing">Citing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#contributing">Contributing</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Notebooks:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_getting_started.html">Quick introduction to generating counterfactual explanations using DiCE</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_feature_importances.html">Estimating local and global feature importance scores using DiCE</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_multiclass_classification_and_regression.html">Generating counterfactuals for multi-class classification and regression models</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_multiclass_classification_and_regression.html#Regression">Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_model_agnostic_CFs.html">Generating counterfactual explanations with any ML model</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_model_agnostic_CFs.html#1.-Independent-random-sampling-of-features">1. Independent random sampling of features</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_model_agnostic_CFs.html#2.-Genetic-Algorithm">2. Genetic Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_model_agnostic_CFs.html#3.-Querying-a-KD-Tree">3. Querying a KD Tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_with_private_data.html">Generating Counterfactual Explanations without access to training data</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_with_advanced_options.html">Advanced options to customize Counterfactual Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_getting_started_feasible.html">Generate feasible counterfactual explanations using a VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/DiCE_getting_started_feasible.html#Adding-feasibility-constraints">Adding feasibility constraints</a></li>
</ul>
<p class="caption"><span class="caption-text">Package:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dice_ml.html">dice_ml package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DiCE</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Diverse Counterfactual Explanations (DiCE) for ML</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/readme.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><a class="reference external" href="https://github.com/interpretml/dice/actions?query=workflow%3A%22Python+package%22"><img alt="BuildStatus" src="https://github.com/interpretml/dice/workflows/Python%20package/badge.svg" /></a> <a class="reference external" href="https://pypi.org/project/dice-ml/"><img alt="PyPiVersion" src="https://img.shields.io/pypi/v/dice-ml" /></a> <a class="reference external" href="https://pypi.org/project/dice-ml/"><img alt="PythonSupport" src="https://img.shields.io/pypi/pyversions/dice-ml" /></a></p>
<div class="section" id="diverse-counterfactual-explanations-dice-for-ml">
<h1>Diverse Counterfactual Explanations (DiCE) for ML<a class="headerlink" href="#diverse-counterfactual-explanations-dice-for-ml" title="Permalink to this headline">¶</a></h1>
<p><em>How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people?</em></p>
<p><a class="reference external" href="https://raam93.github.io/">Ramaravind K. Mothilal</a>, <a class="reference external" href="http://www.amitsharma.in/">Amit Sharma</a>, <a class="reference external" href="https://chenhaot.com/">Chenhao Tan</a></p>
<p><a class="reference external" href="https://arxiv.org/abs/1905.07697">FAT* ‘20 paper</a> | <a class="reference external" href="https://interpretml.github.io/DiCE/">Docs</a> | <a class="reference external" href="https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks">Example Notebooks</a> | Live Jupyter notebook <a class="reference external" href="https://mybinder.org/v2/gh/interpretML/DiCE/master?filepath=docs/source/notebooks"><img alt="Binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
<blockquote>
<div><p><strong>Blog Post</strong>: <a class="reference external" href="https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/">Explanation for ML using diverse counterfactuals</a></p>
<p><strong>Case Studies</strong>: <a class="reference external" href="https://towardsdatascience.com/dice-diverse-counterfactual-explanations-for-hotel-cancellations-762c311b2c64">Towards Data Science</a> (Hotel Bookings) | <a class="reference external" href="https://medium.com/analytics-vidhya/dice-ml-models-with-counterfactual-explanations-for-the-sunk-titanic-30aa035056e0">Analytics Vidhya</a> (Titanic Dataset)</p>
</div></blockquote>
<img alt="Visualizing a counterfactual explanation" class="align-center" src="https://www.microsoft.com/en-us/research/uploads/prod/2020/01/MSR-Amit_1400x788-v3-1blog.gif" />
<p>Explanations are critical for machine learning, especially as machine learning-based systems are being used to inform decisions in societally critical domains such as finance, healthcare, education, and criminal justice.
However, most explanation methods depend on an approximation of the ML model to
create an interpretable explanation. For example,
consider a person who applied for a loan and was rejected by the loan distribution algorithm of a financial company. Typically, the company may provide an explanation on why the loan was rejected, for example, due to “poor credit history”. However, such an explanation does not help the person decide <em>what they do should next</em> to improve their chances of being approved in the future. Critically, the most important feature may not be enough to flip the decision of the algorithm, and in practice, may not even be changeable such as gender and race.</p>
<p>DiCE implements <a class="reference external" href="https://arxiv.org/abs/1711.00399">counterfactual (CF) explanations</a>  that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., <code class="docutils literal notranslate"><span class="pre">you</span> <span class="pre">would</span> <span class="pre">have</span> <span class="pre">received</span> <span class="pre">the</span> <span class="pre">loan</span> <span class="pre">if</span> <span class="pre">your</span> <span class="pre">income</span> <span class="pre">was</span> <span class="pre">higher</span> <span class="pre">by</span> <span class="pre">$10,000</span></code>. In other words, it provides “what-if” explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.</p>
<p>Barring simple linear models, however, it is difficult to generate CF examples that work for any machine learning model. DiCE is based on <a class="reference external" href="https://arxiv.org/abs/1905.07697">recent research</a> that generates CF explanations for any ML model. The core idea is to setup finding such explanations as an optimization problem, similar to finding adversarial examples. The critical difference is that for explanations, we need perturbations that change the output of a machine learning model, but are also diverse and feasible to change. Therefore, DiCE supports generating a set of counterfactual explanations  and has tunable parameters for diversity and proximity of the explanations to the original input. It also supports simple constraints on features to ensure feasibility of the generated counterfactual examples.</p>
<div class="section" id="installing-dice">
<h2>Installing DICE<a class="headerlink" href="#installing-dice" title="Permalink to this headline">¶</a></h2>
<p>DiCE supports Python 3+. The stable version of DiCE is available on <a class="reference external" href="https://pypi.org/project/dice-ml/">PyPI</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install dice-ml
</pre></div>
</div>
<p>To install the latest (dev) version of DiCE and its dependencies, clone this repo and run <cite>pip install</cite> from the top-most folder of the repo:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install -e .
</pre></div>
</div>
<p>If you face any problems, try installing dependencies manually.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip install -r requirements.txt
<span class="c1"># Additional dependendies for deep learning models</span>
pip install -r requirements-deeplearning.txt
<span class="c1"># For running unit tests</span>
pip install -r requirements-test.txt
</pre></div>
</div>
<p>DiCE requires the following packages:</p>
<ul class="simple">
<li><p>numpy</p></li>
<li><p>scikit-learn</p></li>
<li><p>pandas</p></li>
<li><p>h5py</p></li>
<li><p>tqdm</p></li>
<li><p>[optional] tensorflow/pytorch (works with Tensorflow&gt;=1.13)</p></li>
</ul>
</div>
<div class="section" id="getting-started-with-dice">
<h2>Getting started with DiCE<a class="headerlink" href="#getting-started-with-dice" title="Permalink to this headline">¶</a></h2>
<p>With DiCE, generating explanations is a simple three-step  process: train
mode and then invoke DiCE to generate counterfactual examples for any input.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dice_ml</span>
<span class="kn">from</span> <span class="nn">dice_ml.utils</span> <span class="kn">import</span> <span class="n">helpers</span> <span class="c1"># helper functions</span>
<span class="c1"># Dataset for training an ML model</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">dice_ml</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="n">dataframe</span><span class="o">=</span><span class="n">helpers</span><span class="o">.</span><span class="n">load_adult_income_dataset</span><span class="p">(),</span>
                 <span class="n">continuous_features</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;hours_per_week&#39;</span><span class="p">],</span>
                 <span class="n">outcome_name</span><span class="o">=</span><span class="s1">&#39;income&#39;</span><span class="p">)</span>
<span class="c1"># Pre-trained ML model</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">dice_ml</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">dice_ml</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">helpers</span><span class="o">.</span><span class="n">get_adult_income_modelpath</span><span class="p">())</span>
<span class="c1"># DiCE explanation instance</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">dice_ml</span><span class="o">.</span><span class="n">Dice</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>For any given input, we can now generate counterfactual explanations. For
example, the following input leads to class 0 (low income).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query_instance</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;age&#39;</span><span class="p">:</span><span class="mi">22</span><span class="p">,</span>
    <span class="s1">&#39;workclass&#39;</span><span class="p">:</span><span class="s1">&#39;Private&#39;</span><span class="p">,</span>
    <span class="s1">&#39;education&#39;</span><span class="p">:</span><span class="s1">&#39;HS-grad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;marital_status&#39;</span><span class="p">:</span><span class="s1">&#39;Single&#39;</span><span class="p">,</span>
    <span class="s1">&#39;occupation&#39;</span><span class="p">:</span><span class="s1">&#39;Service&#39;</span><span class="p">,</span>
    <span class="s1">&#39;race&#39;</span><span class="p">:</span> <span class="s1">&#39;White&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gender&#39;</span><span class="p">:</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hours_per_week&#39;</span><span class="p">:</span> <span class="mi">45</span><span class="p">}</span>
</pre></div>
</div>
<p>Using DiCE, we can now generate examples that would have been classified as class 1 (high income).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate counterfactual examples</span>
<span class="n">dice_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="o">.</span><span class="n">generate_counterfactuals</span><span class="p">(</span><span class="n">query_instance</span><span class="p">,</span> <span class="n">total_CFs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">desired_class</span><span class="o">=</span><span class="s2">&quot;opposite&quot;</span><span class="p">)</span>
<span class="c1"># Visualize counterfactual explanation</span>
<span class="n">dice_exp</span><span class="o">.</span><span class="n">visualize_as_dataframe</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/interpretml/DiCE/master/docs/_static/getting_started_updated.png"><img alt="List of counterfactual examples" src="https://raw.githubusercontent.com/interpretml/DiCE/master/docs/_static/getting_started_updated.png" style="width: 400px;" /></a>
<p>For more details, check out the <a class="reference external" href="https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks">docs/source/notebooks</a> folder. Here are some example notebooks:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_getting_started.ipynb">Getting Started</a>: Generate CF examples for a <cite>sklearn</cite>, <cite>tensorflow</cite> or <cite>pytorch</cite> binary classifier and compute feature importance scores.</p></li>
<li><p><a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_multiclass_classification_and_regression.ipynb">Explaining Multi-class Classifiers and Regressors</a>: Generate CF explanations for a multi-class classifier or regressor.</p></li>
<li><p><a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_feature_importances.ipynb">Local and Global Feature Importance</a>: Estimate local and global feature importance scores using generated counterfactuals.</p></li>
<li><p><a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_model_agnostic_CFs.ipynb">Providing Constraints on Counterfactual Generation</a>: Specifying which features to vary and their permissible ranges for valid counterfactual examples.</p></li>
</ul>
</div>
<div class="section" id="supported-methods-for-generating-counterfactuals">
<h2>Supported methods for generating counterfactuals<a class="headerlink" href="#supported-methods-for-generating-counterfactuals" title="Permalink to this headline">¶</a></h2>
<p>DiCE can generate counterfactual examples using the following methods.</p>
<p><strong>Model-agnostic methods</strong></p>
<ul class="simple">
<li><p>Randomized sampling</p></li>
<li><p>KD-Tree (for counterfactuals within the training data)</p></li>
<li><p>Genetic algorithm</p></li>
</ul>
<p>See <a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_model_agnostic_CFs.ipynb">model-agnostic notebook</a> for code examples on using these methods.</p>
<p><strong>Gradient-based methods</strong></p>
<ul class="simple">
<li><p>An explicit loss-based method described in <a class="reference external" href="https://arxiv.org/abs/1905.07697">Mothilal et al. (2020)</a> (Default for deep learning models).</p></li>
<li><p>A Variational AutoEncoder (VAE)-based method described in <a class="reference external" href="https://arxiv.org/abs/1912.03277">Mahajan et al. (2019)</a> (see the BaseVAE <a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/notebooks/DiCE_getting_started_feasible.ipynb">notebook</a>).</p></li>
</ul>
<p>The last two methods require a differentiable model, such as a neural network. If you are interested in a specific method, do raise an issue <a class="reference external" href="https://github.com/interpretml/DiCE/issues">here</a>.</p>
</div>
<div class="section" id="supported-use-cases">
<h2>Supported use-cases<a class="headerlink" href="#supported-use-cases" title="Permalink to this headline">¶</a></h2>
<p><strong>Data</strong></p>
<p>DiCE does not need access to the full dataset. It only requires metadata properties for each feature (min, max for continuous features and levels for categorical features). Thus, for sensitive data, the dataset can be provided as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="p">{</span>
                   <span class="s1">&#39;age&#39;</span><span class="p">:[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span>
                   <span class="s1">&#39;workclass&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Government&#39;</span><span class="p">,</span> <span class="s1">&#39;Other/Unknown&#39;</span><span class="p">,</span> <span class="s1">&#39;Private&#39;</span><span class="p">,</span> <span class="s1">&#39;Self-Employed&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;education&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Assoc&#39;</span><span class="p">,</span> <span class="s1">&#39;Bachelors&#39;</span><span class="p">,</span> <span class="s1">&#39;Doctorate&#39;</span><span class="p">,</span> <span class="s1">&#39;HS-grad&#39;</span><span class="p">,</span> <span class="s1">&#39;Masters&#39;</span><span class="p">,</span> <span class="s1">&#39;Prof-school&#39;</span><span class="p">,</span> <span class="s1">&#39;School&#39;</span><span class="p">,</span> <span class="s1">&#39;Some-college&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;marital_status&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Divorced&#39;</span><span class="p">,</span> <span class="s1">&#39;Married&#39;</span><span class="p">,</span> <span class="s1">&#39;Separated&#39;</span><span class="p">,</span> <span class="s1">&#39;Single&#39;</span><span class="p">,</span> <span class="s1">&#39;Widowed&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;occupation&#39;</span><span class="p">:[</span><span class="s1">&#39;Blue-Collar&#39;</span><span class="p">,</span> <span class="s1">&#39;Other/Unknown&#39;</span><span class="p">,</span> <span class="s1">&#39;Professional&#39;</span><span class="p">,</span> <span class="s1">&#39;Sales&#39;</span><span class="p">,</span> <span class="s1">&#39;Service&#39;</span><span class="p">,</span> <span class="s1">&#39;White-Collar&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;race&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Other&#39;</span><span class="p">,</span> <span class="s1">&#39;White&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;gender&#39;</span><span class="p">:[</span><span class="s1">&#39;Female&#39;</span><span class="p">,</span> <span class="s1">&#39;Male&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;hours_per_week&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">99</span><span class="p">]},</span>
         <span class="n">outcome_name</span><span class="o">=</span><span class="s1">&#39;income&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Model</strong></p>
<p>We support pre-trained models as well as training a model. Here’s a simple example using Tensorflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">InteractiveSession</span><span class="p">()</span>
<span class="c1"># Generating train and test data</span>
<span class="n">train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">normalize_data</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">one_hot_encoded_data</span><span class="p">))</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;income&#39;</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train</span><span class="o">.</span><span class="n">columns</span> <span class="o">==</span> <span class="s1">&#39;income&#39;</span><span class="p">]</span>
<span class="c1"># Fitting a dense neural network model</span>
<span class="n">ann_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">ann_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],),</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">))</span>
<span class="n">ann_model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">))</span>
<span class="n">ann_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">ann_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.20</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>

<span class="c1"># Generate the DiCE model for explanation</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">ann_model</span><span class="p">)</span>
</pre></div>
</div>
<p>Check out the <a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_getting_started.ipynb">Getting Started</a> notebook to see code examples on using DiCE with sklearn and PyTorch models.</p>
<p><strong>Explanations</strong></p>
<p>We visualize explanations through a table highlighting the change in features. We plan to support an English language explanation too!</p>
</div>
<div class="section" id="feasibility-of-counterfactual-explanations">
<h2>Feasibility of counterfactual explanations<a class="headerlink" href="#feasibility-of-counterfactual-explanations" title="Permalink to this headline">¶</a></h2>
<p>We acknowledge that not all counterfactual explanations may be feasible for a
user. In general, counterfactuals closer to an individual’s profile will be
more feasible. Diversity is also important to help an individual choose between
multiple possible options.</p>
<p>DiCE provides tunable parameters for diversity and proximity to generate
different kinds of explanations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dice_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="o">.</span><span class="n">generate_counterfactuals</span><span class="p">(</span><span class="n">query_instance</span><span class="p">,</span>
                <span class="n">total_CFs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">desired_class</span><span class="o">=</span><span class="s2">&quot;opposite&quot;</span><span class="p">,</span>
                <span class="n">proximity_weight</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">diversity_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Additionally, it may be the case that some features are harder to change than
others (e.g., education level is harder to change than working hours per week). DiCE allows input of relative difficulty in changing a feature through specifying <em>feature weights</em>. A higher feature weight means that the feature is harder to change than others. For instance, one way is to use the mean absolute deviation from the median as a measure of relative difficulty of changing a continuous feature. By default, DiCE computes this internally and divides the distance between continuous features by the MAD of the feature’s values in the training set. We can also assign different values through the <em>feature_weights</em> parameter.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># assigning new weights</span>
<span class="n">feature_weights</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;hours_per_week&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
<span class="c1"># Now generating explanations using the new feature weights</span>
<span class="n">dice_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="o">.</span><span class="n">generate_counterfactuals</span><span class="p">(</span><span class="n">query_instance</span><span class="p">,</span>
                <span class="n">total_CFs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">desired_class</span><span class="o">=</span><span class="s2">&quot;opposite&quot;</span><span class="p">,</span>
                <span class="n">feature_weights</span><span class="o">=</span><span class="n">feature_weights</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, some features are impossible to change such as one’s age or race. Therefore, DiCE also allows inputting a
list of features to vary.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dice_exp</span> <span class="o">=</span> <span class="n">exp</span><span class="o">.</span><span class="n">generate_counterfactuals</span><span class="p">(</span><span class="n">query_instance</span><span class="p">,</span>
                <span class="n">total_CFs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">desired_class</span><span class="o">=</span><span class="s2">&quot;opposite&quot;</span><span class="p">,</span>
                <span class="n">features_to_vary</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span><span class="s1">&#39;workclass&#39;</span><span class="p">,</span><span class="s1">&#39;education&#39;</span><span class="p">,</span><span class="s1">&#39;occupation&#39;</span><span class="p">,</span><span class="s1">&#39;hours_per_week&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>It also supports simple constraints on
features that reflect practical constraints (e.g., working hours per week
should be between 10 and 50 using the <code class="docutils literal notranslate"><span class="pre">permitted_range</span></code> parameter).</p>
<p>For more details, check out <a class="reference external" href="https://github.com/interpretml/DiCE/blob/master/docs/source/notebooks/DiCE_model_agnostic_CFs.ipynb">this</a> notebook.</p>
</div>
<div class="section" id="the-promise-of-counterfactual-explanations">
<h2>The promise of counterfactual explanations<a class="headerlink" href="#the-promise-of-counterfactual-explanations" title="Permalink to this headline">¶</a></h2>
<p>Being truthful to the model, counterfactual explanations can be useful to all stakeholders for a decision made by a machine learning model that makes decisions.</p>
<ul class="simple">
<li><p><strong>Decision subjects</strong>: Counterfactual explanations can be used to explore actionable recourse for a person based on a decision received by a ML model. DiCE shows decision outcomes with <em>actionable</em> alternative profiles, to help people understand what they could have done to change their model outcome.</p></li>
<li><p><strong>ML model developers</strong>: Counterfactual explanations are also useful for model developers to debug their model for potential problems. DiCE can be used to show CF explanations for a selection of inputs that can uncover if there are any problematic (in)dependences on some features (e.g., for 95% of inputs, changing features X and Y change the outcome, but not for the other 5%). We aim to support aggregate metrics to help developers debug ML models.</p></li>
<li><p><strong>Decision makers</strong>: Counterfactual explanations may be useful to
decision-makers such as doctors or judges who may use ML models to make decisions. For a particular individual, DiCE allows probing the ML model to see the possible changes that lead to a different ML outcome, thus enabling decision-makers to assess their trust in the prediction.</p></li>
<li><p><strong>Decision evaluators</strong>: Finally, counterfactual explanations can be useful
to decision evaluators who may be interested in fairness or other desirable
properties of an ML model. We plan to add support for this in the future.</p></li>
</ul>
</div>
<div class="section" id="roadmap">
<h2>Roadmap<a class="headerlink" href="#roadmap" title="Permalink to this headline">¶</a></h2>
<p>Ideally, counterfactual explanations should balance between a wide range of suggested changes (<em>diversity</em>), and the relative ease of adopting those changes (<em>proximity</em> to the original input), and also follow the causal laws of the world, e.g., one can hardly lower their educational degree or change their race.</p>
<p>We are working on adding the following features to DiCE:</p>
<ul class="simple">
<li><p>Support for using DiCE for debugging machine learning models</p></li>
<li><p>Constructed English phrases (e.g., <code class="docutils literal notranslate"><span class="pre">desired</span> <span class="pre">outcome</span> <span class="pre">if</span> <span class="pre">feature</span> <span class="pre">was</span> <span class="pre">changed</span></code>) and other ways to output the counterfactual examples</p></li>
<li><p>Evaluating feature attribution methods like LIME and SHAP on necessity and sufficiency metrics using counterfactuals (see <a class="reference external" href="https://arxiv.org/abs/2011.04917">this paper</a>)</p></li>
<li><p>Support for Bayesian optimization and other algorithms for generating counterfactual explanations</p></li>
<li><p>Better feasibility constraints for counterfactual generation</p></li>
</ul>
</div>
<div class="section" id="citing">
<h2>Citing<a class="headerlink" href="#citing" title="Permalink to this headline">¶</a></h2>
<p>If you find DiCE useful for your research work, please cite it as follows.</p>
<p>Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan (2020). <strong>Explaining machine learning classifiers through diverse counterfactual explanations</strong>. <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>.</p>
<p>Bibtex:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">mothilal2020dice</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Explaining</span> <span class="n">machine</span> <span class="n">learning</span> <span class="n">classifiers</span> <span class="n">through</span> <span class="n">diverse</span> <span class="n">counterfactual</span> <span class="n">explanations</span><span class="p">},</span>
        <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Mothilal</span><span class="p">,</span> <span class="n">Ramaravind</span> <span class="n">K</span> <span class="ow">and</span> <span class="n">Sharma</span><span class="p">,</span> <span class="n">Amit</span> <span class="ow">and</span> <span class="n">Tan</span><span class="p">,</span> <span class="n">Chenhao</span><span class="p">},</span>
        <span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">2020</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Fairness</span><span class="p">,</span> <span class="n">Accountability</span><span class="p">,</span> <span class="ow">and</span> <span class="n">Transparency</span><span class="p">},</span>
        <span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">607</span><span class="o">--</span><span class="mi">617</span><span class="p">},</span>
        <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2020</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="contributing">
<h2>Contributing<a class="headerlink" href="#contributing" title="Permalink to this headline">¶</a></h2>
<p>This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a class="reference external" href="https://cla.microsoft.com">https://cla.microsoft.com</a>.</p>
<p>When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p>This project has adopted the <a class="reference external" href="https://opensource.microsoft.com/codeofconduct/">Microsoft Open Source Code of Conduct</a>.
For more information see the <a class="reference external" href="https://opensource.microsoft.com/codeofconduct/faq/">Code of Conduct FAQ</a> or
contact <a class="reference external" href="mailto:opencode&#37;&#52;&#48;microsoft&#46;com">opencode<span>&#64;</span>microsoft<span>&#46;</span>com</a> with any additional questions or comments.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="notebooks/nb_index.html" class="btn btn-neutral float-right" title="&lt;no title&gt;" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Diverse Counterfactual Explanations (DiCE) for ML" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Ramaravind, Amit, Chenhao

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>